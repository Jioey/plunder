#pragma once

#include <iostream>
#include <random>
#include <fstream>
#include <iomanip>

#define PRECISION 6
#define epsilon 10E-6
#define SEED 3256+2585 // Random seed

using namespace std;

random_device rd;
default_random_engine gen(SEED);









enum HA { // High-level actions
    ACC, // Constant acceleration
    DEC, // Constant deceleration
    CON  // No acceleration
};

struct LA { // Low-level actions
    double acc; // actual acceleration
};

struct Obs { // State observations
    double pos; // position
    double vel; // velocity
};


class RobotPhysics {
    private:
    public:

    double accMax;
    double decMax;
    double vMax;
    double target;

    normal_distribution<double> accErrDistr;

    double a = 0; // acceleration
    double v = 0; // velocity
    double x = 0; // displacement
    HA ha;

    RobotPhysics(double _accMax, double _decMax, double _vMax, double _target, double _accErrStdev): 
        accMax(_accMax), decMax(_decMax), vMax(_vMax), target(_target) {
            accErrDistr = normal_distribution<double>(0.0, _accErrStdev);
        }

    /*
     * Given a current high-level action, apply a motor controller and update observed state. Runs once per time step
     */
    void updatePhysics(double t_step){
        double vPrev = v;
        double xPrev = x;

        // Select some action (acceleration)
        a = ha == DEC ? decMax :
                            (ha == ACC ? accMax : 0);
        
        // Induce some error
        a += accErrDistr(gen);
        
        // Update velocity and displacement accordingly
        v = vPrev + a * t_step;

        if(v < epsilon){ // Round to 0
            v = 0;
        }

        if(abs(v - vMax) < epsilon){ // Round to vMax
            v = vMax;
        }

        if(abs(x - target) < epsilon){ // Round to target
            x = target;
        }

        x = xPrev + (v + vPrev)/2 * t_step;
    }

    /*
     * Robot has reached target and is at rest. End simulation.
     */
    bool finished(){
        return v < epsilon && x >= target - epsilon;
    }

    void reset(){
        a = 0;
        v = 0;
        x = 0;
        ha = CON;
    }
};








class RobotASP {
    private:

    // Return the distance this robot would travel before stopping if it began decelerating immediately
    double DistTraveled(double v, double dec){
        return - v * v / (2 * dec);
    }

    double logistic(double midpoint, double steepness, double input){
        return 1.0 / (1.0 + exp(-steepness * (input - midpoint)));
    }

    bool sampleDiscrete(double probTrue){
        double rv = ((double) rand())/RAND_MAX;
        return rv <= probTrue;
    }



    /*
     * This is a hand-crafted action-selection policy.
     */
    void changeHA_Hand(RobotPhysics r){
        double xToTarget = r.target - r.x;                                  // distance to the target
        bool cond1 = r.v - r.vMax >= 0;                                     // is at max velocity (can no longer accelerate)
        bool cond2 = xToTarget - DistTraveled(r.v, r.decMax) < epsilon;     // needs to decelerate or else it will pass target

        if(cond2)               r.ha = DEC;
        if(cond1 && !cond2)     r.ha = CON;
        if(!cond1 && !cond2)    r.ha = ACC;
    }

    /*
     * This is a probabilistic hand-crafted action-selection policy that more accurately reflects human behavior.
     */
    void changeHA_Hand_prob(RobotPhysics r){
        double xToTarget = r.target - r.x;                            // distance to the target
        bool cond1smooth = sampleDiscrete(logistic(r.vMax*0.1, -50.0/r.vMax, r.vMax-r.v));
        bool cond2smooth = sampleDiscrete(logistic(r.target*0.1, -50.0/r.target, xToTarget - DistTraveled(r.v, r.decMax)));

        if(cond2smooth)                     r.ha = DEC;
        if(cond1smooth && !cond2smooth)     r.ha = CON;
        if(!cond1smooth && !cond2smooth)    r.ha = ACC;
    }

    /*
     * This is an action-selection policy generated by LDIPS, given data points with no error. It has 100% consistency with hand-crafted policy.
     */
    void changeHA_LDIPS(RobotPhysics r){
        // Copy paste below
        if(r.ha == ACC && DistTraveled(r.v, r.decMax) + r.x - r.target >= -2.320007)
            r.ha = DEC;
        else if(r.ha == CON && DistTraveled(r.v, r.decMax) - DistTraveled(r.vMax, r.decMax) >= -150.000000 && DistTraveled(r.vMax, r.decMax) + r.x - r.target >= -0.095000)
            r.ha = DEC;
        else if(r.ha == ACC && r.v - r.vMax >= -0.025000 && r.x - r.target >= -499.975006)
            r.ha = CON;
        else if(r.ha == CON && r.vMax - r.v >= 0.000000)
            r.ha = ACC;
        else if(r.ha == DEC && r.v >= -1.000000)
            r.ha = DEC;
        else if(r.ha == ACC && r.x >= -0.997500)
            r.ha = ACC;
        else if(r.ha == CON && r.v >= 0.000000 && DistTraveled(r.vMax, r.decMax) - r.x >= -128.399994)
            r.ha = CON;
    }

    /*
     * This is an action-selection policy generated by LDIPS, with error.
     */
    void changeHA_LDIPS_error(RobotPhysics r){
        // Copy paste below
        if(r.ha == CON && DistTraveled(r.v, r.decMax) - DistTraveled(r.vMax, r.decMax) >= 9.714069)
            r.ha = CON;
        else if(r.ha == DEC && DistTraveled(r.v, r.decMax) - r.target >= 11.458965)
            r.ha = CON;
        else if(r.ha == CON && r.x + r.x + r.x - r.target >= 35.615170)
            r.ha = DEC;
        else if(r.ha == ACC && DistTraveled(r.v, r.decMax) + r.target >= 535.545532)
            r.ha = CON;
        else if(r.ha == CON)
            r.ha = ACC;
        else if(r.ha == ACC && r.x - r.target + DistTraveled(r.v, r.decMax) >= -0.138184)
            r.ha = DEC;
        else if(r.ha == DEC && DistTraveled(r.v, r.decMax) - r.x - r.x >= -49.242615)
            r.ha = ACC;
        else if(r.ha == DEC)
            r.ha = DEC;
        else if(r.ha == ACC)
            r.ha = ACC;
    }

    /*
     * This ASP forms a triangular velocity-time graph instead of a trapezoidal one.
    */
    void changeHA_Hand_triangle(RobotPhysics r){
        if(r.x < r.target / 2){
            r.ha = ACC;
        } else {
            r.ha = DEC;
        }
    }

    /*
     * Randomly transitions to an incorrect high-level action with specified probability
     */
    void putErrorIntoHA(RobotPhysics r){
        double v = ((double) rand()) /RAND_MAX;
        int haDif = v < haProbCorrect ? 0 :
                                (v < 1-(1-haProbCorrect)/2) ? 1 : 2;
        r.ha = static_cast<HA>((r.ha + haDif)%3);
    }


    public:

    int modelNum = 0;
    bool wantErrHA = true;
    double haProbCorrect = 0.8;
    bool useTree;

    RobotASP(modelNum, wantErrHA, haProbCorrect, useTree){

    }
    

    /*
     * Transition robot high-level action based on current global state. Runs once per time step
     */
    void changeHA(RobotPhysics r){

        if(modelNum == 0) {
            changeHA_Hand(r);
        } else if(modelNum == 1){
            changeHA_LDIPS(r);
        } else if(modelNum == 2){
            changeHA_LDIPS_error(r);
        } else if(modelNum == 3){
            changeHA_Hand_prob(r);
        } else{
            exit(1);
        }

        if(wantErrHA) putErrorIntoHA(r);
    }

    
};
