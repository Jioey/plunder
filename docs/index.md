## Programmatic Imitation Learning from Unlabeled and Noisy Demonstrations


<video src='assets/plunder.mov' width=180/>


Imitation Learning (IL) is a promising paradigm for teaching robots to perform novel tasks using demonstrations. Most existing approaches for IL utilize neural networks (NN), however, these methods suffer from several well-known limitations: they 1) require large amounts of training data, 2) are hard to interpret, and 3) are hard to refine and adapt.


There is an emerging interest in Programmatic Imitation Learning (PIL), which offers significant promise in addressing the above limitations. In PIL, the learned policy is represented in a programming language, making it amenable to interpretation and adaptation to novel settings. However, state-of-the-art PIL algorithms assume access to action labels and struggle to learn from noisy real-world demonstrations.


In this paper, we propose PLUNDER, a novel PIL algorithm that integrates a probabilistic program synthesizer in an iterative Expectation-Maximization (EM) framework to address these shortcomings.  Unlike existing PIL approaches, PLUNDER synthesizes *probabilistic*
programmatic policies that are particularly well-suited for modeling the uncertainties inherent in real-world demonstrations. Our approach leverages an EM loop to simultaneously infer the missing action labels and the most likely probabilistic policy.

Our method is robust against actuation errors in the demonstrations, and the synthesized probabilistic policies intuitively reflect uncertainties commonly found in human demonstrations. We benchmark PLUNDER against several established IL techniques, and demonstrate its superiority across five challenging imitation learning tasks under noise. PLUNDER policies achieve $95\%$ accuracy in matching the given demonstrations, outperforming the next best baseline by $19\%$. Additionally, policies generated by PLUNDER successfully complete the tasks $17\%$ more frequently than the nearest baseline.