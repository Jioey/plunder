
# PLUNDER: Probabilistic Program Synthesis for Learning from Unlabeled and Noisy Demonstrations

![](assets/asp_8.gif)

Our goal is to synthesize a programmatic state machine policy from time-series data while simultaneously inferring a set of high-level action labels. 


<link rel="stylesheet" href="assets/style.css">

<div class="icon-container">
          <!-- ArXiv Paper URL -->
          <a href="https://arxiv.org/abs/2309.13549" class="icon-button arxiv" target="_blank" title="ArXiv Paper">
            <i class="fas fa-scroll"></i>
            <span>Paper</span>
          </a>
           <!-- Dataset URL -->
          <a href="https://doi.org/10.18738/T8/BBOQMV" class="icon-button dataset" target="_blank" title="Dataset">
            <i class="fas fa-database"></i>
            <span>Dataset</span>
          </a>
          <!-- GitHub Profile -->
          <a href="https://github.com/ut-amrl/coda-devkit" class="icon-button github" target="_blank" title="GitHub Profile">
            <i class="fab fa-github"></i>
            <span>DevKit</span>
          </a>
          <!-- GitHub Repository -->
          <a href="https://github.com/ut-amrl/coda-models" class="icon-button github" target="_blank" title="GitHub Repository">
            <i class="fab fa-github"></i>
            <span>Models</span>
          </a>
          <!-- YouTube Video -->
          <a href="https://youtu.be/WSvygHg5aYM?si=Vt3RDAPxVGlLIl0B" class="icon-button youtube" target="_blank" title="YouTube Video">
            <i class="fab fa-youtube"></i>
            <span>Presentation</span>
          </a>
        </div>

---
## Abstract


Imitation Learning (IL) is a promising paradigm for teaching robots to perform novel tasks using demonstrations. Most existing approaches for IL utilize neural networks (NN), however, these methods suffer from several well-known limitations: they 1) require large amounts of training data, 2) are hard to interpret, and 3) are hard to refine and adapt.


There is an emerging interest in Programmatic Imitation Learning (PIL), which offers significant promise in addressing the above limitations. In PIL, the learned policy is represented in a programming language, making it amenable to interpretation and adaptation to novel settings. However, state-of-the-art PIL algorithms assume access to action labels and struggle to learn from noisy real-world demonstrations.


In this paper, we propose PLUNDER, a novel PIL algorithm that integrates a probabilistic program synthesizer in an iterative Expectation-Maximization (EM) framework to address these shortcomings.  Unlike existing PIL approaches, PLUNDER synthesizes *probabilistic*
programmatic policies that are particularly well-suited for modeling the uncertainties inherent in real-world demonstrations. Our approach leverages an EM loop to simultaneously infer the missing action labels and the most likely probabilistic policy.

Our method is robust against actuation errors in the demonstrations, and the synthesized probabilistic policies intuitively reflect uncertainties commonly found in human demonstrations. We benchmark PLUNDER against several established IL techniques, and demonstrate its superiority across five challenging imitation learning tasks under noise. PLUNDER policies achieve $95\%$ accuracy in matching the given demonstrations, outperforming the next best baseline by $19\%$. Additionally, policies generated by PLUNDER successfully complete the tasks $17\%$ more frequently than the nearest baseline.


---
## Related Resources
PLUNDER (codebase): [https://github.com/ut-amrl/plunder](https://github.com/ut-amrl/plunder)

AMRL Google Drive (presentations and videos): [https://drive.google.com/drive/folders/1QaKtIvmKhZjxIwY9ANSPpjYl0teoNW5S?usp=share_link](https://drive.google.com/drive/folders/1QaKtIvmKhZjxIwY9ANSPpjYl0teoNW5S?usp=share_link)

Publication: [https://arxiv.org/abs/2303.01440](https://arxiv.org/abs/2303.01440)

---
## Description

Our system is a *discrete-time Markov process* defined by:
   - an **action space** $A$ = a set of discrete action labels $a \in A$
     - Ex: $a \in$ {ACC, DEC, CON}
   - a **low-level observation space** $Z$ = a continuous domain of low-level observations $z \in Z$: controlled joystick directives, motor inputs, etc.
     - Ex: $z = acc \in \mathbb{R}$, where $acc$ is the acceleration
   - a **state space** $S$ = a continuous domain of constants or variables $c, y \in S$.
     - Ex: $c = accMax \in \mathbb{R}, y = pos \in \mathbb{R}$
   - an **action-selection policy (ASP)** $\pi: A \times S \rightarrow A$ that maps the current action label and the current state to the next action label
   - an **observation model** $O: A \rightarrow distr(Z)$ that maps discrete action labels to a distribution over low-level observations via discrete motor controllers

---
## Overall problem formulation:
### Inputs
We know the problem domain $A, Z, S$, as well as the observation model $O$. We are given a set of **demonstrations**, which are defined simply as trajectories with the action labels missing, i.e. $s_{1:t}$ and $z_{1:t}$.

### Outputs
We would like to:
1. Infer the values of the action labels in the demonstrations ($a_{1:t}$)
2. Synthesize an ASP that is maximally consistent with the demonstrations ($\pi^*$)

---
## Dependencies & Setup
See **pips/**. 
In addition, this project requires Scipy: https://scipy.org/install/.
If you wish to run the highway environment yourself, you'll need highway-env and its dependencies: https://highway-env.readthedocs.io/en/latest/installation.html
If you wish to run the robotic arm environment yourself, you'll also need panda-gym and its dependencies: https://panda-gym.readthedocs.io/en/latest/index.html

---
# How to run examples
We have provided five example tasks: 1D-target, 2D-highway-env, 2D-merge, panda-pick-place, and panda-stack.

To run these tasks:
1. Go into the Makefile and set the variable *target_dir* to the desired folder (default set to 1D-target).
2. Run **make** to build the project.
3. Run **make em** (for 1D-target) or **make emng** (for the other tasks) to run PLUNDER.

Please see each of these folders for an extended usage guide.

---
# Further configuration
To setup a custom environment, you will need to do the following:
- Create a new folder to house your problem domain. 
- In that directory, create the files **domain.h, robot.h, settings.h,** and **emdips_operations.json**. 
- In **domain.h**, define your action space, observation space, and state space.
- In **robot.h**, define your observation model.
- In **settings.h**, tune the desired parameters and I/O paths.
- In **emdips_operations**, define your desired operations (plus, minus, times, etc). See *pips/* for general tips and guidelines for defining operations and a list of existing operations.

If you need to simulate your own demonstrations, you can also use our interface to:
- Define the ground-truth ASP and the physical simulation model in **robot.h**.
- Set the desired demonstration initial states in another file **robotSets.h**.

An example setup is defined in *1D-target*; it may be easier to copy paste that folder and work from there.

Then, you can use *make* commands to run the project:
- **make** to build the project. (Go into the Makefile and set the variable *target_dir* to the desired folder, then run *make*.)
- **make em** to run the full EM Synthesis algorithm, including simulating demonstrations
- **make emng** to run the EM Synthesis algorithm, without simulating demonstrations
- **make plt** to plot the algorithm outputs and store them in png format
- **make clean, make clear_data, make purge** to delete all build files, to clear all data/plots/trajectories, or both
- **make snapshot** to archive current settings and output files to a given folder

Other *make* commands which are not commonly used alone:
- **make gen** to run only the simulation
- **make pf** to run only the particle filter (E-step)
- **make settings** to compile settings

---
## Project Organization
This project is roughly split into the following components:

- **simulation/** - for simulating demonstrations given a ground-truth ASP
- **particleFilter/** (expectation step) - runs a particle filter to get a set of most likely action labels
- **pips/** (maximization step) - runs a program synthesizer to generate the program that is maximally consistent with the given action labels
- **synthesis/** - runs the EM-loop, alternating between expectation and maximization steps
- **system.h** - fully defines the discrete-time Markov process given *domain.h* and *robot.h*
- **utils.h** - useful functions for general use
- **includes.h** - all include statements for tidiness
- **translateSettings.cpp** - converts settings.h into a text file (settings.txt) for easy Python interpretation