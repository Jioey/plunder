


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
<link href='//fonts.googleapis.com/css?family=Lustria|Lato:400,700,400italic|Playfair+Display:700,400italic' rel='stylesheet' type='text/css'>
<link rel="stylesheet" type="text/css" href="assets/style.css">
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


---
<center>
<h1 class="title"><span class="smallcaps">Plunder</span>: Probabilistic Program Synthesis for Learning from Unlabeled and Noisy Demonstrations</h1>

<p class="authors">Jimmy Xin, Linus Zheng, Jiayi Wei, Kia Rahmani, Jarrett Holtz, Isil Dillig, Joydeep Biswas</p>
</center>

![](assets/asp_8.gif)

Our goal is to synthesize a programmatic state machine policy from time-series data while simultaneously inferring a set of high-level action labels. 

<div class="icon-container">

  <a href="https://arxiv.org/abs/2303.01440" class="icon-button arxiv" target="_blank" title="ArXiv Paper">
    <i class="fas fa-scroll"></i>
    <span>Paper</span>
  </a>

  <a href="https://github.com/ut-amrl/plunder" class="icon-button github" target="_blank" title="GitHub Repository">
    <i class="fab fa-github"></i>
    <span>GitHub</span>
  </a>


  <a href="https://drive.google.com/drive/folders/1QaKtIvmKhZjxIwY9ANSPpjYl0teoNW5S" class="icon-button drive" target="_blank" title="Supplementary Videos and Tables">
    <i class="fab fa-google-drive"></i>
    <span>Supplements</span>
  </a>

  <a href="https://www.youtube.com/watch?v=Fy1P_46c54A" class="icon-button youtube" target="_blank" title="YouTube Video">
    <i class="fab fa-youtube"></i>
    <span>Demo</span>
  </a>


</div>

---
## Abstract

<span class="smallcaps">Plunder</span> is a novel Programmatic Imitation Learning (PIL) algorithm that integrates a probabilistic program synthesizer in an iterative Expectation-Maximization (EM) framework. Our method is robust against actuation errors in  demonstrations, and the synthesized probabilistic policies intuitively reflect uncertainties commonly found in human demonstrations. We benchmark <span class="smallcaps">Plunder</span> against several established IL techniques, and demonstrate its superiority across five challenging imitation learning tasks under noise. <span class="smallcaps">Plunder</span> policies achieve $95\%$ accuracy in matching the given demonstrations, outperforming the next best baseline by $19\%$. Additionally, policies generated by <span class="smallcaps">Plunder</span> successfully complete the tasks $17\%$ more frequently than the nearest baseline.

![](assets/stack.gif)